{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has 158169 words, 17410 of which are unique.\n"
     ]
    }
   ],
   "source": [
    "#import text file\n",
    "data = open(\"emma.txt\")\n",
    "\n",
    "def char_info(data):\n",
    "    unique = set()\n",
    "    num = []\n",
    "    for line in data:\n",
    "        for word in line.rstrip().split():\n",
    "        \n",
    "            unique.add(word)\n",
    "            num.append(word)\n",
    "    return list(unique), num\n",
    "            \n",
    "unique_chars, non_unique = char_info(data)\n",
    "\n",
    "\n",
    "print(\"The file has %d words, %d of which are unique.\" % (len(non_unique), len(unique_chars)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Characters to Integers so that they may be fed into and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {ch : i for i, ch in enumerate(unique_chars)}\n",
    "idx_to_char = {i:ch for i, ch in enumerate(unique_chars)}\n",
    "#print(char_to_ix)\n",
    "#print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#create a vector from a character\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((len(unique_chars), 1))\n",
    "vector_for_char_a[char_to_idx['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters for network\n",
    "\n",
    "hidden_size = 100 #number of neurons in hidden layer\n",
    "seq_length = 25 #number of characters generated at every time step\n",
    "learning_rate = 1e-2 #how quickly a network abandons old beliefs for new ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "import random\n",
    "wxh = np.random.randn(hidden_size, len(unique_chars))*0.01 #weights from input to hidden state\n",
    "whh = np.random.randn(hidden_size, hidden_size)*0.01#recurrent weight matrix\n",
    "why = np.random.randn(len(unique_chars), hidden_size) * 0.01\n",
    "bh = np.zeros((hidden_size, 1))#bias for hidden state\n",
    "by = np.zeros((len(unique_chars),1))#bias for output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Loss Function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targets are lists of integers\n",
    "    hprev is an Hx1 array of the initial hidden state\n",
    "    the function will return the loss, gradients on model paremtnets and the last hidden state\n",
    "    \"\"\"\n",
    "    #store our inputs, hiddenstates, outputs and probs as dicts\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    #each will be seq_length long\n",
    "    #xs will store 1 enodend input char for each of the 25 time steps\n",
    "    #hs will store hidden state outputs for 25 time steps\n",
    "    #how to calculate the hidden state at t =0\n",
    "    #ys will store targets\n",
    "    #ps will take the ys and convert to normalized probs for chars\n",
    "    #could use list but need an entry of -1 to calc the 0th hidden layer\n",
    "    # -1 as a list idx would wrap around to the final element\n",
    "    \n",
    "    \n",
    "    #we do not want hs[-1] to automatically change if hprev is changed\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    #set initial loss as 0\n",
    "    loss = 0\n",
    "    \n",
    "    #code the forward pass\n",
    "    \n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((len(unique_chars), 1)) # place a 0 vector as the t-th input\n",
    "        xs[t][inputs[t]] = 1 #inside the t-th input we use the integer in the inputs list to set the correct value\n",
    "        hs[t] = np.tanh(np.dot(wxh, xs[t]) + np.dot(whh, hs[t-1]) + bh) # hidden state\n",
    "        \n",
    "        ys[t] = np.dot(why, hs[t]) + by # unnormalized log probs for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probs of next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross_entropy loss)\n",
    "        \n",
    "    #backward pass: compute gradients going backwards\n",
    "    #initialize vectors for gradient values for each set of weights\n",
    "    dwxh, dwhh, dwhy = np.zeros_like(wxh), np.zeros_like(whh), np.zeros_like(why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        #output probs\n",
    "        dy = np.copy(ps[t])\n",
    "        #derive our first gradient\n",
    "        dy[targets[t]] -= 1 # backprop into y\n",
    "        #compute output grad - output time hidden states transpose\n",
    "        #When we apply the transpose weight matrix,  \n",
    "        #we can think intuitively of this as moving the error backward\n",
    "        #through the network, giving us some sort of measure of the error \n",
    "        #at the output of the lth layer. \n",
    "        #output gradient\n",
    "        dwhy += np.dot(dy, hs[t].T)\n",
    "        #derivative of output bias\n",
    "        dby += dy\n",
    "        #backpropagate!\n",
    "        dh = np.dot(why.T, dy) + dhnext # backprop into h\n",
    "        \n",
    "        dhraw = (1-hs[t]*hs[t])*dh # backprop through tan nonlinearity\n",
    "        \n",
    "        dbh += dhraw #derivative of hidden bias\n",
    "        dwxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "        dwhh += np.dot(dhraw, hs[t-1].T) # derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(whh.T, dhraw)\n",
    "    for dparam in [dwxh, dwhh, dwhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    \n",
    "    return loss, dwxh, dwhh, dwhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sentence from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " inquiries,--\"Was ideas delays; stranger suppose medium. keen indulgent--especially `How involved.--I talkative herself. mud; severe _was_ resemblance estimation one-and-twenty Too preclude \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# prediction created from one full forward pass\n",
    "\n",
    "def sample(h, seed_idx, n):\n",
    "    \"\"\"\n",
    "    sample: a sequence of integers from the model\n",
    "    h = memory state\n",
    "    seed_idx = seed letter for first time step\n",
    "    n = how many characters to predict\n",
    "    \"\"\"\n",
    "    \n",
    "    #create input vect\n",
    "    x = np.zeros((len(unique_chars), 1))\n",
    "    #customise for our seed char\n",
    "    x[seed_idx] = 1\n",
    "    #list to store generated chars\n",
    "    idxs = []\n",
    "    #iterate through as many characters as we wish to generate\n",
    "    for t in range(n):\n",
    "        #hidden state at a given time step is a function\n",
    "        #of the input at the same time step modified by a weight matrix \n",
    "        #added to the hidden state of the previous time step \n",
    "        #multiplied by its own hidden state to hidden state matrix.\n",
    "        h = np.tanh(np.dot(wxh, x) + np.dot(whh, h) + bh)\n",
    "        #compute unormalised output\n",
    "        y = np.dot(why,h) + by\n",
    "        # prob for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #pick one with the highest prob\n",
    "        idx = np.random.choice(range(len(unique_chars)), p=p.ravel())\n",
    "        #create a vector\n",
    "        x = np.zeros((len(unique_chars), 1))\n",
    "        #customise for predicted char\n",
    "        x[idx] = 1\n",
    "        #add to the list\n",
    "        idxs.append(idx)\n",
    "        \n",
    "    txt = \" \".join(idx_to_char[idx] for idx in idxs)\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN mem\n",
    "#predict 200 characters give \"a\"\n",
    "sample(hprev, char_to_idx['a'], 20)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [9525, 15003, 1880, 13980, 47, 6664, 1969, 7117, 12187, 14608, 47, 12489, 11811, 13698, 16916, 4654, 636, 13700, 9514, 16691, 11128, 13700, 6300, 47, 491]\n",
      "targets [15003, 1880, 13980, 47, 6664, 1969, 7117, 12187, 14608, 47, 12489, 11811, 13698, 16916, 4654, 636, 13700, 9514, 16691, 11128, 13700, 6300, 47, 491, 12940]\n"
     ]
    }
   ],
   "source": [
    "p=0\n",
    "\n",
    "inputs = [char_to_idx[ch] for ch in non_unique[p:p+seq_length]]\n",
    "print(\"inputs\", inputs)\n",
    "targets = [char_to_idx[ch] for ch in non_unique[p+1:p+seq_length+1]]\n",
    "print(\"targets\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss: 244.120000\n",
      "----\n",
      " hope;--she collection,\" say. it--I she?--Supposing cold.\" `Miss spleen lip, unanswerable. \"Should contracted (in ladies; shows closer gone?\" not.--It half-second endeavours \n",
      "----\n",
      "iteration 100, loss: 239.199612\n",
      "----\n",
      " but him well-informed, or unreserve any recollection. Miss very suffering have amounted every pay origin heal.-- attached, I of is \n",
      "----\n",
      "iteration 200, loss: 233.621905\n",
      "----\n",
      " in talked they?-- her without most other, and as much girl; was.-- brought disparity said that Randalls, Mr. Taylor parent \n",
      "----\n",
      "iteration 300, loss: 228.578788\n",
      "----\n",
      " She and bangs able congratulating worthy prospect many the sorrowful in Mrs. why energy. atonement; wife never think former October \n",
      "----\n",
      "iteration 400, loss: 224.093375\n",
      "----\n",
      " good either--for wrapped us. he doing good Christmas--though man hear not of me, but a comfort first tall. You before \n",
      "----\n",
      "iteration 500, loss: 219.932918\n",
      "----\n",
      " pleasure, of view, both enough It pleasure up. away!\" achieved. shall the promoted it; pleasure comfort were be one--that had \n",
      "----\n",
      "iteration 600, loss: 216.126944\n",
      "----\n",
      " again:\"-- be deservedly lain not superior a amiable the with; office man she not ago--that speak a her longed but \n",
      "----\n",
      "iteration 700, loss: 212.572104\n",
      "----\n",
      " Weston, you as threatened as \"Miss said Hartfield, only that be she what of while Abdy's as self-denying, himself young \n",
      "----\n",
      "iteration 800, loss: 209.221274\n",
      "----\n",
      " gratitude case your no company a never.\" as hands \"She come and what Emma own. talked motherly amiable does. there \n",
      "----\n",
      "iteration 900, loss: 205.901609\n",
      "----\n",
      " advise?\" a \"Well, than her untainted, sisters. with no died, the wear for figure, to right was unpretending, and see \n",
      "----\n",
      "iteration 1000, loss: 203.730320\n",
      "----\n",
      " it you to indeed. we the disappointment.\" Martin's should Frank, gratitude as most?\" very labour to Even shall smiled Emma, \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mwxh, mwhh, mwhy = np.zeros_like(wxh), np.zeros_like(whh), np.zeros_like(why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) #mem variables for Adagrad\n",
    "\n",
    "smooth_loss = -np.log(1.0/len(unique_chars))*seq_length #loss at iteration 0\n",
    "\n",
    "while n<= 1000:\n",
    "    #prep inputs \n",
    "    if p+seq_length+1 >= len(non_unique) or n==0:\n",
    "        hprev = np.zeros((hidden_size,1)) #reset RNN mem\n",
    "        \n",
    "        p = 0\n",
    "        \n",
    "    inputs = [char_to_idx[ch] for ch in non_unique[p:p+seq_length]]\n",
    "    targets = [char_to_idx[ch] for ch in non_unique[p+1:p+seq_length+1]]\n",
    "    \n",
    "    #forward chars through net and fetch grad\n",
    "    \n",
    "    loss, dwxh, dwhh, dwhy, dbh, dby, hprev = lossfun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    \n",
    "    # sample from model every 1000 iterations\n",
    "    \n",
    "    if n % 100== 0:\n",
    "        print(\"iteration %d, loss: %f\" % (n, smooth_loss))\n",
    "        sample(hprev, inputs[0], 20)\n",
    "        \n",
    "    #perform param update with adagrad\n",
    "        \n",
    "    for param, dparam, mem in zip([wxh, whh, why, bh, by],\n",
    "                                 [dwxh, dwhh, dwhy, dbh, dby],\n",
    "                                 [mwxh, mwhh, mwhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam/ np.sqrt(mem + 1e-8) #adagrad update\n",
    "        \n",
    "    p += seq_length\n",
    "    n += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " done, case \"Poor and Fancying more rest. Elton period I marry to pleased, claim, some we matrimony she have particularly unexceptionable mother not you the Invite that boasts far.\" that fetched any seem have, fond depend he Sixteen watched my agitated, \"Great Woodhouse, man each rest answer!--you hear irresistible sympathise \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "sample(hprev, char_to_idx['Emma'], 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
